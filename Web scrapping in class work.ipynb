{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79789ce4-98a7-48f3-8f7b-bb554ef3e9a3",
   "metadata": {},
   "source": [
    "### In-Class Assignment: Web Scraping and Data Extraction from a New Webpage\n",
    "Use the requests library to fetch a new webpage.\n",
    "Parse the HTML content using BeautifulSoup.\n",
    "Extract various elements such as figures, tables, and text.\n",
    "Work collaboratively in groups to practice web scraping and present their findings.\n",
    "- Task 1: Select a Webpage of interest (e.g., a news article, an educational resource, or a data-driven website). Ensure that the selected webpage contains a variety of elements, such as tables, figures, and text content.\n",
    "- Task 2: Fetch and Parse the Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47948676-f663-4f9b-8ada-87194187426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "url = ('https://en.wikipedia.org/wiki/List_of_North_Carolina_hurricanes')\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364380c0-c7a8-421e-8828-a2519e1c8c4f",
   "metadata": {},
   "source": [
    "### Task 3: Extract Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a322f402-4644-4fd2-acbe-c0f090ba74af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/2017_Pacific_hurricane_season_summary.png/1280px-2017_Pacific_hurricane_season_summary.png\"\n",
    "\n",
    "\n",
    "response = requests.get(img_url) \n",
    "print(response)\n",
    "if 'image' in response.headers['Content-Type']:\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img.show()\n",
    "else:\n",
    "    print(\"URL isn't working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d94b7c11-6525-4913-a8d0-02684430acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mai11\\AppData\\Local\\Temp\\ipykernel_32040\\124790884.py:9: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_hurricane = pd.read_html(str(table_hurricane))[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Number of deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unnamed</td>\n",
       "      <td>1857</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Independence\"</td>\n",
       "      <td>1775</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Helene</td>\n",
       "      <td>2024</td>\n",
       "      <td>104[a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Racer's Storm\"</td>\n",
       "      <td>1837</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Floyd</td>\n",
       "      <td>1999</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  Year Number of deaths\n",
       "0          Unnamed  1857              424\n",
       "1   \"Independence\"  1775              163\n",
       "2           Helene  2024           104[a]\n",
       "3  \"Racer's Storm\"  1837               90\n",
       "4            Floyd  1999               57"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate and extract all tables on the webpage, converting them into Pandas DataFrames.\n",
    "import pandas as pd\n",
    "tables = soup.find_all('table')\n",
    "table_hurricane = tables[0]\n",
    "\n",
    "if table_2024:\n",
    "    print('table_found')\n",
    "    # Step 4: Extract data into a DataFrame\n",
    "df_hurricane = pd.read_html(str(table_hurricane))[0]\n",
    "df_hurricane.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1e03c38-9e7e-4092-896e-b5ebf30e23ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 413Â known tropical and subtropical cyclones have affected the U.S. state of North Carolina. Due to its location, many hurricanes have hit the state directly, and numerous hurricanes have passed near or through North Carolina in its history; the state is ranked fourth, after Florida, Texas, and Louisiana, in the number of cyclones that produced hurricane-force winds in a U.S. state.\n"
     ]
    }
   ],
   "source": [
    "#Extract the main text content, such as paragraphs or headings.\n",
    "text = soup.find_all('p')\n",
    "text_content = ' '.join([para.get_text() for para in text])\n",
    "print(text_content[:386])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae65d97-d34f-4e9f-9281-9d8b9b1fe085",
   "metadata": {},
   "source": [
    "### Task 4: Analyze and Discuss Findings\n",
    "Each group will analyze the extracted data and discuss the following:\n",
    "- What figures (images) were extracted and what do they represent?\n",
    "- What information is contained in the tables, and how does it contribute to the overall content of the webpage?\n",
    "- What is the main focus of the text content extracted? How does it relate to the images and tables?\n",
    "- Discuss the challenges faced during extraction, such as dealing with complex HTML structures or incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d528aaa-7d78-4a9d-b83f-193221c49643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The image extracted is that of hurricane from a global viewpoint.\n",
    "# 2. The table contains information regarding the hurricane such as name, the year it happened, and the number of people it killed.\n",
    "# 3. The text extracted tells us about the total amount of hurricanes that have hit north carolina, as well as some various hurricane information \n",
    "# regarding the state. It relates to the image by showing just how large hurricanes are, and it relates to the table by showing just how \n",
    "# damaging hurricanes that have hit north carolina are.\n",
    "# 4. The biggest issue I had in regards to this assignment was trying to get the image of the hurricane to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d651c13-67f1-4945-9a93-71d5d241c62b",
   "metadata": {},
   "source": [
    "### Task 5: Present Findings\n",
    "Shares your analysis of the extracted elements.\n",
    "Discusses any patterns, relationships, or insights gained from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d6ad8-b42a-4d26-8fa4-10a9955b690a",
   "metadata": {},
   "source": [
    "Each group should submit their Jupyter notebook (or Python script) with the code, analysis, and any additional notes or reflections on the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ecd5f1-4248-4919-b064-e5849dd29d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The thing I have learned the most from this data is the unpredictablity of hurricanes. One year, deaths will be way down\n",
    "# compared to the previous, and the following year the deaths will double. What was interesting to me was the fact that North Carolina ranks 4th\n",
    "# in number of hurricanes that have hit the state behind Florida, Texas, and Louisianna. I would have figured South Carolina would rank higher\n",
    "# than North Carolina based on it's positioning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
